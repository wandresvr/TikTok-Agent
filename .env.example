# Configuración de TikTok Live Agent

# Usuario de TikTok a escuchar (sin @)
TIKTOK_USERNAME=saximt

# Credenciales para enviar mensajes (opcional)
# Para obtener estas credenciales:
# 1. Abre TikTok en tu navegador y inicia sesión
# 2. Abre las herramientas de desarrollador (F12)
# 3. Ve a la pestaña "Application" > "Cookies"
# 4. Busca las cookies "sessionid" y "tt-target-idc"
# 5. Copia sus valores aquí

TIKTOK_SESSION_ID=tu_session_id_aqui
TIKTOK_TT_TARGET_IDC=tu_tt_target_idc_aqui

# Envío de mensajes: API Euler (pago) o screen scraping (gratis)
# USE_BROWSER_SENDER=true  → Envía mensajes con un navegador (Playwright). Gratis.
#   La primera vez se abrirá el navegador: inicia sesión en TikTok y cierra.
#   El perfil se guarda en BROWSER_USER_DATA_DIR (por defecto ./browser-profile).
# USE_BROWSER_SENDER=false → Envía por Euler Stream (requiere plan premium).
USE_BROWSER_SENDER=false
# BROWSER_USER_DATA_DIR=./browser-profile
# BROWSER_HEADLESS=false   → Ver el navegador al enviar (útil para depurar)
# BROWSER_DEBUG_PORT=9222  → Puerto para prueba con node scripts/test-browser-sender.js
# CHROME_PATH=C:\Program Files (x86)\Google\Chrome\Application\chrome.exe  → Ruta a Chrome (para --launch-chrome)

# API Key de Euler Stream (solo si USE_BROWSER_SENDER=false)
# Para enviar mensajes necesitas un plan premium de Euler Stream
# Obtén tu API key en: https://www.eulerstream.com/pricing
EULER_API_KEY=

# Modelo de Ollama a usar (opcional)
# Si no se especifica, se detectará automáticamente (prioridad: llama3.2 > phi3 > mistral > qwen2 > llama3 > gemma > llama).
# Para ver modelos disponibles: ollama list
#
# Recomendados para este proyecto (clasificación JSON + respuestas cortas en español):
#   - llama3.2:3b  (rápido, buen JSON, ~2GB)  → ollama pull llama3.2:3b
#   - phi3         (rápido, instrucciones, ~2GB) → ollama pull phi3
#   - llama3.2:1b  (muy rápido, poca RAM, ~700MB)
#   - mistral      (buen equilibrio)
# Si tienes poca RAM: llama3.2:1b, phi3:mini, tinyllama.
# Modelos grandes (más RAM): llama3, llama3.2 (sin tag).
OLLAMA_MODEL=

# Timeout para respuestas de Ollama (clasificación + generación de texto), en milisegundos.
# 0 = sin límite. Ej: 60000 = 1 min, 120000 = 2 min. Si no se define, analyze() usa 2 min por defecto.
OLLAMA_RESPONSE_TIMEOUT_MS=120000

# Respuesta periódica: cada cuántos ms Ollama envía un mensaje al chat (saludo, pide canciones, etc.).
# 120000 = cada 2 min. 0 = desactivado.
OLLAMA_PERIODIC_INTERVAL_MS=120000

# Envío automático de mensajes al chat (true/false).
# true  = el bot envía al chat las respuestas generadas por Ollama.
# false = Ollama sigue generando respuestas (se ven en consola como "Respuesta (no enviada)") pero no se escriben en el chat.
# Por defecto: true (si no se define, se envían mensajes).
ENABLE_AUTO_SEND=true

# Mostrar en consola los comentarios de otros usuarios del chat (true/false).
# true  = ver todos los comentarios en consola
# false = solo ver los que el bot procesa o responde
SHOW_OTHER_COMMENTS=false
