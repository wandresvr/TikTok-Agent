// llm/ollamaClient.js

// Cache para evitar mostrar el mismo error muchas veces
let errorShown = false;
let configuredModelUnavailableWarned = false;
let availableModel = null;

// Rate limiting: tiempo m√≠nimo entre peticiones (ms)
let lastRequestTime = 0;
const MIN_REQUEST_INTERVAL = 1500; // 1.5 segundos entre peticiones

// Contador de errores 500 consecutivos
let consecutive500Errors = 0;
const MAX_500_ERRORS = 3;

// Modelo configurado desde .env
const CONFIGURED_MODEL = process.env.OLLAMA_MODEL?.trim() || null;

/**
 * Obtiene la lista de modelos disponibles en Ollama
 */
async function getAvailableModels() {
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 3000);
    
    const res = await fetch('http://localhost:11434/api/tags', {
      method: 'GET',
      signal: controller.signal
    });
    
    clearTimeout(timeoutId);
    
    if (res.ok) {
      const data = await res.json();
      const models = data.models?.map(m => m.name) || [];
      return models;
    }
    return [];
  } catch (error) {
    return [];
  }
}

/**
 * Verifica si un modelo espec√≠fico est√° disponible
 */
async function isModelAvailable(modelName) {
  const models = await getAvailableModels();
  return models.includes(modelName);
}

/**
 * Encuentra un modelo disponible
 * Prioridad: 1. Modelo configurado en .env, 2. Detecci√≥n autom√°tica
 */
async function findAvailableModel() {
  if (availableModel) return availableModel;
  
  // Si hay un modelo configurado en .env, usarlo
  if (CONFIGURED_MODEL) {
    const isAvailable = await isModelAvailable(CONFIGURED_MODEL);
    if (isAvailable) {
      availableModel = CONFIGURED_MODEL;
      return CONFIGURED_MODEL;
    } else {
      if (!configuredModelUnavailableWarned) {
        configuredModelUnavailableWarned = true;
        const models = await getAvailableModels();
        console.warn(`‚ö†Ô∏è Modelo configurado "${CONFIGURED_MODEL}" no est√° disponible`);
        if (models.length === 0) {
          console.warn(`üí° Modelos disponibles: Ninguno. ¬øOllama est√° corriendo? Prueba: ollama serve && ollama pull phi3:mini`);
        } else {
          console.warn(`üí° Modelos disponibles: ${models.join(', ')}`);
        }
        console.warn(`üí° Usando detecci√≥n autom√°tica...`);
      }
    }
  }
  
  // Detecci√≥n autom√°tica como fallback
  const models = await getAvailableModels();
  if (models.length === 0) return null;
  
  // Prioridad seg√∫n uso: clasificaci√≥n JSON + respuestas cortas ‚Üí modelos r√°pidos y que sigan instrucciones
  const preferred =
    models.find(m => m.includes('llama3.2')) ||  // 1b/3b: r√°pidos, buen JSON
    models.find(m => m.includes('phi3') || m === 'phi') ||
    models.find(m => m.includes('mistral')) ||
    models.find(m => m.includes('qwen2')) ||
    models.find(m => m.includes('llama3')) ||
    models.find(m => m.includes('gemma')) ||
    models.find(m => m.includes('llama')) ||
    models[0];
  
  availableModel = preferred;
  return preferred;
}

/**
 * Espera un tiempo antes de hacer la siguiente petici√≥n (rate limiting)
 */
async function waitForRateLimit() {
  const now = Date.now();
  const timeSinceLastRequest = now - lastRequestTime;
  
  const waitTime = consecutive500Errors >= MAX_500_ERRORS 
    ? MIN_REQUEST_INTERVAL * (consecutive500Errors + 1) 
    : MIN_REQUEST_INTERVAL;
  
  if (timeSinceLastRequest < waitTime) {
    const wait = waitTime - timeSinceLastRequest;
    await new Promise(resolve => setTimeout(resolve, wait));
  }
  
  lastRequestTime = Date.now();
}

async function analyze(text) {
  try {
    // Encontrar modelo disponible
    const model = await findAvailableModel();
    if (!model) {
      if (!errorShown) {
        console.error('‚ùå No se encontraron modelos disponibles en Ollama');
        console.error('üí° Recomendado para este proyecto (r√°pido + JSON): ollama pull llama3.2:3b o ollama pull phi3');
        errorShown = true;
      }
      return JSON.stringify({ type: 'normal', song: null });
    }

    // Rate limiting
    await waitForRateLimit();

    // Timeout: OLLAMA_RESPONSE_TIMEOUT_MS (0 = sin l√≠mite). Por defecto 2 min para dar tiempo a Ollama.
    const timeoutMs = parseInt(process.env.OLLAMA_RESPONSE_TIMEOUT_MS || '120000', 10) || 0;
    const controller = new AbortController();
    const timeoutId = timeoutMs > 0 ? setTimeout(() => controller.abort(), timeoutMs) : null;

    const res = await fetch('http://localhost:11434/api/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      signal: timeoutMs > 0 ? controller.signal : undefined,
      body: JSON.stringify({
        model: model,
        format: 'json',        // üî• ESTO ES LA CLAVE
        stream: false,
        messages: [
          {
            role: 'system',
            content: `
Eres un moderador experto de lives musicales.

Objetivo:
Mantener el enganche del live y ayudar al streamer.

Reglas de comportamiento:

El streamer canta canciones por pedido.

Reglas de comportamiento:

El streamer canta canciones por pedido.

Mensajes muy cortos y directos.

Si el p√∫blico pide una canci√≥n, responde con UN dato curioso breve sobre esa canci√≥n.

El dato debe ser simple, popular y f√°cil de entender.

Si no hay pedidos, pide canciones, tap tap ‚ù§Ô∏è o peque√±os apoyos.

Puedes usar emojis.

Eres moderador, no streamer.

Restricciones OBLIGATORIAS:

Responde EXCLUSIVAMENTE en JSON v√°lido.

El campo "message" NO PUEDE superar 50 caracteres.

Cuenta los caracteres antes de responder.

Si no puedes cumplir el l√≠mite, responde exactamente:

üéµ Pide tu canci√≥n ‚ù§Ô∏è"

No expliques nada. No agregues texto fuera del JSON.
`
          },
          {
            role: 'user',
            content: `
Clasifica este mensaje.

Devuelve exactamente este formato:
{
  "type": "request|vote|rating|normal|spam",
  "song": null | "artista - canci√≥n"
}

Mensaje:
"${text}"
`
          }
        ]
      })
    });

    if (timeoutId) clearTimeout(timeoutId);

    if (!res.ok) {
      if (res.status === 500) {
        consecutive500Errors++;
        // Siempre intentar leer y mostrar el cuerpo del error para diagnosticar
        let errorBody = '';
        try {
          errorBody = await res.text();
        } catch (e) {
          // Ignorar
        }
        if (!errorShown) {
          console.warn(`‚ö†Ô∏è Error 500 de Ollama (analyze)`);
          if (errorBody) {
            console.warn(`üìã Respuesta de Ollama: ${errorBody.substring(0, 500)}${errorBody.length > 500 ? '...' : ''}`);
          }
          if (errorBody && (errorBody.includes('unable to allocate') || errorBody.includes('buffer') || errorBody.includes('memory'))) {
            console.error(`üí• Posible problema de memoria`);
            console.error(`üí° El modelo "${model}" puede requerir m√°s RAM. Prueba: ollama pull llama3.2:1b`);
          } else if (errorBody && (errorBody.includes('not found') || errorBody.includes('load'))) {
            console.error(`üí° El modelo puede no estar cargado. Prueba en otra terminal: ollama run ${model}`);
          }
          errorShown = true;
        }
      } else if (res.status === 404 && !errorShown) {
        console.error(`‚ùå Modelo "${model}" no encontrado (404)`);
        console.error(`üí° Modelos disponibles: ${(await getAvailableModels()).join(', ') || 'Ninguno'}`);
        console.error(`üí° Recomendado: ollama pull llama3.2:3b o ollama pull phi3`);
        errorShown = true;
      } else if (res.status !== 404 && res.status !== 500 && !errorShown) {
        console.error(`‚ùå Error en respuesta de Ollama (analyze): ${res.status} ${res.statusText}`);
        errorShown = true;
      }
      return JSON.stringify({ type: 'normal', song: null });
    }

    // Resetear contador de errores 500 si la petici√≥n fue exitosa
    consecutive500Errors = 0;
    errorShown = false;
    const data = await res.json();
    return data.message?.content || JSON.stringify({ type: 'normal', song: null });
  } catch (error) {
    if (error.name === 'AbortError') {
      if (!errorShown) {
        const timeoutMs = parseInt(process.env.OLLAMA_RESPONSE_TIMEOUT_MS || '120000', 10) || 0;
        console.error(`‚è±Ô∏è Timeout: Ollama tard√≥ demasiado en responder${timeoutMs > 0 ? ` (>${timeoutMs / 1000}s)` : ''}. Puedes aumentar OLLAMA_RESPONSE_TIMEOUT_MS en .env o usar 0 para sin l√≠mite.`);
        errorShown = true;
      }
    } else if (error.code === 'ECONNREFUSED' || error.message?.includes('fetch failed')) {
      if (!errorShown) {
        console.error('‚ùå No se puede conectar a Ollama en http://localhost:11434');
        console.error('üí° Verifica que Ollama est√© corriendo: ollama serve');
        errorShown = true;
      }
    } else if (!errorShown) {
      console.error(`‚ùå Error analizando mensaje: ${error.message || error}`);
      errorShown = true;
    }
    // Retornar un JSON por defecto si falla
    return JSON.stringify({ type: 'normal', song: null });
  }
}

module.exports = { analyze };
