// llm/responseGenerator.js
const fs = require('fs');
const path = require('path');

// Cache para evitar mostrar el mismo error muchas veces
let responseErrorShown = false;
let configuredModelUnavailableWarned = false;
let availableModel = null;

// Rate limiting: tiempo m√≠nimo entre peticiones (ms)
let lastRequestTime = 0;
const MIN_REQUEST_INTERVAL = 2000; // 2 segundos entre peticiones

// Contador de errores 500 consecutivos
let consecutive500Errors = 0;
const MAX_500_ERRORS = 3; // Despu√©s de 3 errores 500, esperar m√°s tiempo

// Modelo configurado desde .env
const CONFIGURED_MODEL = process.env.OLLAMA_MODEL?.trim() || null;

/**
 * Obtiene la lista de modelos disponibles en Ollama
 */
async function getAvailableModels() {
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 3000);
    
    const res = await fetch('http://localhost:11434/api/tags', {
      method: 'GET',
      signal: controller.signal
    });
    
    clearTimeout(timeoutId);
    
    if (res.ok) {
      const data = await res.json();
      const models = data.models?.map(m => m.name) || [];
      return models;
    }
    return [];
  } catch (error) {
    return [];
  }
}

/**
 * Verifica si un modelo espec√≠fico est√° disponible
 */
async function isModelAvailable(modelName) {
  const models = await getAvailableModels();
  return models.includes(modelName);
}

/**
 * Encuentra un modelo disponible
 * Prioridad: 1. Modelo configurado en .env, 2. Detecci√≥n autom√°tica
 */
async function findAvailableModel() {
  if (availableModel) return availableModel;
  
  // Si hay un modelo configurado en .env, usarlo
  if (CONFIGURED_MODEL) {
    const isAvailable = await isModelAvailable(CONFIGURED_MODEL);
    if (isAvailable) {
      availableModel = CONFIGURED_MODEL;
      return CONFIGURED_MODEL;
    } else {
      if (!configuredModelUnavailableWarned) {
        configuredModelUnavailableWarned = true;
        const models = await getAvailableModels();
        console.warn(`‚ö†Ô∏è Modelo configurado "${CONFIGURED_MODEL}" no est√° disponible`);
        if (models.length === 0) {
          console.warn(`üí° Modelos disponibles: Ninguno. ¬øOllama est√° corriendo? Prueba: ollama serve && ollama pull phi3:mini`);
        } else {
          console.warn(`üí° Modelos disponibles: ${models.join(', ')}`);
        }
        console.warn(`üí° Usando detecci√≥n autom√°tica...`);
      }
    }
  }
  
  // Detecci√≥n autom√°tica como fallback
  const models = await getAvailableModels();
  if (models.length === 0) return null;
  
  // Prioridad seg√∫n uso: respuestas cortas en espa√±ol ‚Üí modelos r√°pidos y conversacionales
  const preferred =
    models.find(m => m.includes('llama3.2')) ||
    models.find(m => m.includes('phi3') || m === 'phi') ||
    models.find(m => m.includes('mistral')) ||
    models.find(m => m.includes('qwen2')) ||
    models.find(m => m.includes('llama3')) ||
    models.find(m => m.includes('gemma')) ||
    models.find(m => m.includes('llama')) ||
    models[0];
  
  availableModel = preferred;
  return preferred;
}

/**
 * Verifica si Ollama est√° disponible
 */
async function checkOllamaAvailable() {
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 3000); // Timeout de 3 segundos
    
    const res = await fetch('http://localhost:11434/api/tags', {
      method: 'GET',
      signal: controller.signal
    });
    
    clearTimeout(timeoutId);
    return res.ok;
  } catch (error) {
    return false;
  }
}

/**
 * Espera un tiempo antes de hacer la siguiente petici√≥n (rate limiting)
 */
async function waitForRateLimit() {
  const now = Date.now();
  const timeSinceLastRequest = now - lastRequestTime;
  
  // Si hay muchos errores 500, esperar m√°s tiempo
  const waitTime = consecutive500Errors >= MAX_500_ERRORS 
    ? MIN_REQUEST_INTERVAL * (consecutive500Errors + 1) 
    : MIN_REQUEST_INTERVAL;
  
  if (timeSinceLastRequest < waitTime) {
    const wait = waitTime - timeSinceLastRequest;
    await new Promise(resolve => setTimeout(resolve, wait));
  }
  
  lastRequestTime = Date.now();
}

/**
 * Intenta hacer una petici√≥n con retry para errores 500
 */
async function makeRequestWithRetry(model, userMessage, context, maxRetries = 2) {
  console.log(`üîÑ [makeRequestWithRetry] Iniciando petici√≥n con modelo: ${model}, maxRetries: ${maxRetries}`);
  
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      console.log(`‚è≥ [makeRequestWithRetry] Intento ${attempt + 1}/${maxRetries + 1} - Esperando rate limit...`);
      await waitForRateLimit();
      
      console.log(`üì° [makeRequestWithRetry] Enviando petici√≥n a Ollama...`);
      const timeoutMs = parseInt(process.env.OLLAMA_RESPONSE_TIMEOUT_MS || '0', 10) || 0;
      const controller = new AbortController();
      const timeoutId = timeoutMs > 0 ? setTimeout(() => controller.abort(), timeoutMs) : null;

      const res = await fetch('http://localhost:11434/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        signal: timeoutMs > 0 ? controller.signal : undefined,
        body: JSON.stringify({
          model: model,
          stream: false,
          messages: [
            {
              role: 'system',
              content: `Eres un asistente amigable y divertido en un live de TikTok musical.
Responde de forma breve, natural y en espa√±ol.
Mant√©n las respuestas cortas (m√°ximo 2-3 l√≠neas).
S√© amigable, usa emojis ocasionalmente, pero no abuses de ellos.
Si alguien pregunta por canciones, menciona las m√°s pedidas si las hay.
Si el mensaje es un saludo, responde amigablemente.
Si es una pregunta, responde de forma √∫til pero concisa.`
            },
            {
              role: 'user',
              content: `Mensaje del usuario: "${userMessage}"
${context.topSongs ? `Canciones m√°s pedidas: ${context.topSongs.join(', ')}` : ''}

Genera una respuesta natural y breve para este mensaje.`
            }
          ]
        })
      });

      if (timeoutId) clearTimeout(timeoutId);

      console.log(`üì• [makeRequestWithRetry] Respuesta recibida: ${res.status} ${res.statusText}`);

      if (res.ok) {
        // Resetear contador de errores 500 si la petici√≥n fue exitosa
        consecutive500Errors = 0;
        responseErrorShown = false;
        const data = await res.json();
        console.log(`‚úÖ [makeRequestWithRetry] Datos recibidos:`, JSON.stringify(data).substring(0, 300));
        const content = data.message?.content?.trim();
        if (!content) {
          console.warn(`‚ö†Ô∏è [makeRequestWithRetry] Respuesta OK pero sin contenido. Data:`, JSON.stringify(data).substring(0, 200));
        } else {
          console.log(`‚úÖ [makeRequestWithRetry] Contenido extra√≠do: "${content.substring(0, 100)}..."`);
        }
        return content || null;
      }

      // Manejar errores 500 con retry
      if (res.status === 500) {
        consecutive500Errors++;
        if (attempt < maxRetries) {
          const backoffTime = Math.min(1000 * Math.pow(2, attempt), 5000); // Exponential backoff, max 5s
          console.warn(`‚ö†Ô∏è Error 500 de Ollama, reintentando en ${backoffTime}ms... (intento ${attempt + 1}/${maxRetries + 1})`);
          await new Promise(resolve => setTimeout(resolve, backoffTime));
          continue;
        } else {
          console.error(`‚ùå Error 500 de Ollama despu√©s de ${maxRetries + 1} intentos`);
          console.error(`üí° Ollama puede estar sobrecargado. Espera unos segundos antes de la siguiente petici√≥n.`);
          return null;
        }
      }

      // Otros errores HTTP
      if (res.status === 404) {
        console.error(`‚ùå Modelo "${model}" no encontrado (404)`);
        const availableModels = await getAvailableModels();
        console.error(`üí° Modelos disponibles: ${availableModels.join(', ') || 'Ninguno'}`);
        console.error(`üí° Recomendado: ollama pull llama3.2:3b o ollama pull phi3`);
      } else if (res.status === 500) {
        // Error 500 puede ser por falta de memoria
        console.error(`‚ùå Error 500 de Ollama (sin retry disponible en este punto)`);
        try {
          const errorBody = await res.text();
          if (errorBody.includes('unable to allocate') || errorBody.includes('buffer')) {
            console.error(`\nüí• PROBLEMA DE MEMORIA DETECTADO`);
            console.error(`üí° El modelo "${model}" requiere m√°s memoria RAM de la disponible`);
            console.error(`üí° Soluciones:`);
            console.error(`   1. Usa un modelo m√°s peque√±o: ollama pull llama3.2:1b o ollama pull phi3:mini`);
            console.error(`   2. Cierra otras aplicaciones para liberar RAM`);
            console.error(`   3. Configura OLLAMA_MODEL en .env con un modelo m√°s peque√±o`);
            console.error(`   4. Modelos peque√±os recomendados: llama3.2:1b, phi, tinyllama\n`);
          } else {
            console.error(`üí° Detalles del error: ${errorBody.substring(0, 300)}`);
          }
        } catch (e) {
          // Ignorar si no se puede leer el cuerpo
        }
      } else if (res.status !== 404 && res.status !== 500) {
        console.error(`‚ùå Error en respuesta de Ollama: ${res.status} ${res.statusText}`);
        // Intentar leer el cuerpo del error para m√°s informaci√≥n
        try {
          const errorBody = await res.text();
          console.error(`üí° Detalles del error: ${errorBody.substring(0, 200)}`);
        } catch (e) {
          // Ignorar si no se puede leer el cuerpo
        }
      }
      return null;
    } catch (error) {
      if (error.name === 'AbortError') {
        if (attempt < maxRetries) {
          const backoffTime = 2000;
          await new Promise(resolve => setTimeout(resolve, backoffTime));
          continue;
        }
        if (!responseErrorShown) {
          const timeoutMs = parseInt(process.env.OLLAMA_RESPONSE_TIMEOUT_MS || '0', 10) || 0;
          console.error(`‚è±Ô∏è Timeout: Ollama tard√≥ demasiado en responder${timeoutMs > 0 ? ` (>${timeoutMs / 1000}s)` : ''}. Puedes aumentar OLLAMA_RESPONSE_TIMEOUT_MS o poner 0 para sin l√≠mite.`);
          responseErrorShown = true;
        }
      }
      return null;
    }
  }
  return null;
}

/**
 * Genera una respuesta usando el LLM bas√°ndose en el mensaje recibido
 * y el contexto del live (como las canciones m√°s pedidas)
 */
async function generateResponse(userMessage, context = {}) {
  try {
    console.log(`üîç [generateResponse] Iniciando generaci√≥n de respuesta...`);
    
    // Verificar si Ollama est√° disponible
    const ollamaAvailable = await checkOllamaAvailable();
    if (!ollamaAvailable) {
      if (!responseErrorShown) {
        console.error('‚ö†Ô∏è Ollama no est√° disponible en http://localhost:11434');
        console.error('üí° Aseg√∫rate de que Ollama est√© corriendo: ollama serve');
        responseErrorShown = true;
      }
      return null;
    }
    console.log(`‚úÖ [generateResponse] Ollama est√° disponible`);

    // Encontrar modelo disponible
    const model = await findAvailableModel();
    if (!model) {
      if (!responseErrorShown) {
        console.error('‚ùå No se encontraron modelos disponibles en Ollama');
        console.error('üí° Recomendado para respuestas en vivo: ollama pull llama3.2:3b o ollama pull phi3');
        responseErrorShown = true;
      }
      return null;
    }
    console.log(`‚úÖ [generateResponse] Modelo encontrado: ${model}`);

    const result = await makeRequestWithRetry(model, userMessage, context);
    if (result) {
      console.log(`‚úÖ [generateResponse] Respuesta generada exitosamente`);
    } else {
      console.log(`‚ö†Ô∏è [generateResponse] No se pudo generar respuesta (makeRequestWithRetry retorn√≥ null)`);
    }
    return result;
  } catch (error) {
    console.error(`‚ùå [generateResponse] Excepci√≥n capturada:`, error.message || error);
    if (error.code === 'ECONNREFUSED' || error.message?.includes('fetch failed')) {
      if (!responseErrorShown) {
        console.error('‚ùå No se puede conectar a Ollama en http://localhost:11434');
        console.error('üí° Verifica que Ollama est√© corriendo: ollama serve');
        responseErrorShown = true;
      }
    } else if (!responseErrorShown) {
      console.error(`‚ùå Error generando respuesta: ${error.message || error}`);
      responseErrorShown = true;
    }
    return null;
  }
}

// Cola de mensajes pendientes de respuesta
let responseQueue = [];
let isProcessingQueue = false;
const MAX_QUEUE_SIZE = 5; // M√°ximo de mensajes en cola
const RESPONSE_COOLDOWN = 5000; // 5 segundos entre respuestas
let lastResponseTime = 0;

/**
 * Procesa la cola de respuestas
 */
async function processResponseQueue() {
  if (isProcessingQueue || responseQueue.length === 0) return;
  
  isProcessingQueue = true;
  
  while (responseQueue.length > 0) {
    const { msg, topSongs, tiktokConnection, allowSend = true } = responseQueue.shift();
    
    // Verificar cooldown
    const now = Date.now();
    const timeSinceLastResponse = now - lastResponseTime;
    if (timeSinceLastResponse < RESPONSE_COOLDOWN) {
      const waitTime = RESPONSE_COOLDOWN - timeSinceLastResponse;
      await new Promise(resolve => setTimeout(resolve, waitTime));
    }
    
    try {
      console.log(`üí≠ Procesando respuesta para: "${msg.text.substring(0, 50)}..."`);
      const response = await generateResponse(msg.text, { topSongs });
      if (response) {
        let sent = false;
        if (allowSend && tiktokConnection && tiktokConnection.sendMessage) {
          console.log(`üì§ Enviando respuesta: "${response}"`);
          sent = await tiktokConnection.sendMessage(response);
          if (sent) {
            console.log(`‚úÖ Respuesta enviada exitosamente`);
            lastResponseTime = Date.now();
          } else {
            console.log(`‚ùå No se pudo enviar la respuesta`);
          }
        } else {
          console.log(`üí¨ Respuesta (no enviada): "${response}"`);
        }
        saveResponseToCsvIfEnabled(msg.user, msg.text, response, sent);
      } else {
        console.log(`‚ö†Ô∏è No se gener√≥ respuesta del LLM`);
      }
    } catch (e) {
      console.error(`‚ùå Error procesando respuesta:`, e.message);
    }
  }
  
  isProcessingQueue = false;
}

/**
 * Agrega un mensaje a la cola de respuestas.
 * allowSend: si false, Ollama genera la respuesta pero no se env√≠a al chat (ENABLE_AUTO_SEND=false).
 */
function queueResponse(msg, topSongs, tiktokConnection, allowSend = true) {
  // Limitar el tama√±o de la cola
  if (responseQueue.length >= MAX_QUEUE_SIZE) {
    console.log(`‚ö†Ô∏è Cola de respuestas llena, ignorando mensaje: "${msg.text.substring(0, 30)}..."`);
    return;
  }
  
  responseQueue.push({ msg, topSongs, tiktokConnection, allowSend });
  processResponseQueue();
}

/**
 * Determina si un mensaje merece una respuesta
 * Ahora m√°s selectivo para evitar responder a todo
 */
function shouldRespond(msg) {
  const text = msg.text.toLowerCase().trim();
  
  // No responder a mensajes muy cortos o muy largos
  if (text.length < 5 || text.length > 150) {
    return false;
  }

  // No responder a mensajes que son solo emojis o s√≠mbolos
  if (/^[\s\W]+$/.test(text.replace(/[a-z0-9]/gi, ''))) {
    return false;
  }

  // Responder solo a preguntas directas (con ?)
  const hasQuestionMark = text.includes('?');
  
  // Responder a saludos espec√≠ficos (m√°s restrictivo)
  const specificGreetings = ['hola', 'hi', 'hello', 'buenas noches', 'buenos d√≠as', 'buenas tardes'];
  const hasGreeting = specificGreetings.some(greeting => {
    const regex = new RegExp(`^${greeting}[\\s!.,]*$`, 'i');
    return regex.test(text);
  });
  
  // Responder a menciones directas al streamer/DJ
  const hasDirectMention = /@\w+|streamer|dj|minh|@minh/i.test(text);
  
  // Responder a preguntas espec√≠ficas sobre canciones/m√∫sica
  const musicQuestions = ['qu√© canci√≥n', 'qu√© m√∫sica', 'qu√© tema', 'pon', 'ponme', 'play'];
  const hasMusicQuestion = musicQuestions.some(q => text.includes(q));
  
  // Solo responder si cumple criterios espec√≠ficos
  return hasQuestionMark || hasGreeting || (hasDirectMention && text.length > 10) || hasMusicQuestion;
}

/**
 * Escapa un valor para CSV (comillas dobles y saltos de l√≠nea).
 */
function escapeCsvValue(val) {
  if (val == null) return '';
  const s = String(val).replace(/"/g, '""');
  return /[",\n\r]/.test(s) ? `"${s}"` : s;
}

/**
 * Si SAVE_RESPONSES_CSV=true, append una fila al CSV en RESPONSES_CSV_PATH.
 * user: nombre del usuario, userMessage: mensaje que dispar√≥ la respuesta, response: texto del bot, sent: si se envi√≥ al chat.
 */
function saveResponseToCsvIfEnabled(user, userMessage, response, sent) {
  if (process.env.SAVE_RESPONSES_CSV !== 'true') return;
  const csvPath = process.env.RESPONSES_CSV_PATH?.trim();
  if (!csvPath) return;
  try {
    const fullPath = path.resolve(csvPath);
    const header = 'fecha,usuario,mensaje_usuario,respuesta_bot,enviado';
    const needsHeader = !fs.existsSync(fullPath);
    const row = [
      new Date().toISOString(),
      escapeCsvValue(user),
      escapeCsvValue(userMessage),
      escapeCsvValue(response),
      sent ? 'si' : 'no'
    ].join(',');
    const line = (needsHeader ? header + '\n' : '') + row + '\n';
    fs.appendFileSync(fullPath, line, 'utf8');
  } catch (e) {
    console.error('‚ùå Error guardando respuesta en CSV:', e.message);
  }
}

module.exports = { generateResponse, shouldRespond, queueResponse, saveResponseToCsvIfEnabled };
